{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6920701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import spikeinterface as si  # import core only\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.preprocessing as spre\n",
    "import spikeinterface.sorters as ss\n",
    "import spikeinterface.postprocessing as spost\n",
    "import spikeinterface.qualitymetrics as sqm\n",
    "import spikeinterface.comparison as sc\n",
    "import spikeinterface.exporters as sexp\n",
    "import spikeinterface.curation as scur\n",
    "import spikeinterface.widgets as sw\n",
    "import seaborn as sns\n",
    "from spikeinterface import WaveformExtractor, extract_waveforms\n",
    "from spikeinterface import extract_waveforms\n",
    "from probeinterface import Probe, ProbeGroup\n",
    "from probeinterface.plotting import plot_probe, plot_probe_group\n",
    "from probeinterface import generate_multi_columns_probe\n",
    "from probeinterface import write_probeinterface, read_probeinterface\n",
    "from probeinterface import write_prb, read_prb\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d55b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2270d7a",
   "metadata": {},
   "source": [
    "# Sorting Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e51a227",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# only need to run the first time, then can be commented out\n",
    "!git clone https://github.com/flatironinstitute/ironclust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8def84d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set path to ironclust folder\n",
    "ss.IronClustSorter.set_ironclust_path(r'D:\\MEA_Analysis\\SI_files/ironclust')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e6ce9a",
   "metadata": {},
   "source": [
    "# Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca31466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reads from directory\n",
    "directory_folders = r\"D:\\MEA_Analysis\\SI_files\"\n",
    "sub_folders = [name for name in os.listdir(directory_folders) if os.path.isdir(os.path.join(directory_folders, name)) and name.endswith(\".raw\")]\n",
    "\n",
    "print('directory_folders: ', directory_folders)\n",
    "print('sub_folders: ', sub_folders)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac94681",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the directory path \n",
    "directory_path = r\"D:\\MEA_Analysis\\SI_files\" \n",
    "\n",
    "# List all files in the directory\n",
    "mea_files = [file for file in os.listdir(directory_path) if file.endswith('.raw')]\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 5\n",
    "\n",
    "# Initialize a counter for the number of files read\n",
    "file_counter = 0\n",
    "\n",
    "# Process files in batches\n",
    "for i in range(0, len(mea_files), batch_size):\n",
    "    batch_files = mea_files[i:i+batch_size]\n",
    "    extractors = []\n",
    "\n",
    "    # Loop through the list of files in the current batch and create recording extractors\n",
    "    for mea_file in batch_files:\n",
    "        file_path = os.path.join(directory_path, mea_file)\n",
    "        recording = se.MCSRawRecordingExtractor(file_path)\n",
    "        extractors.append(recording) \n",
    "\n",
    "        # Increment the file counter\n",
    "        file_counter += 1\n",
    "\n",
    "        # Print some information about the loaded recording and the current file count\n",
    "        print(f\"File: {mea_file}, Num channels: {recording.get_num_channels()}, Duration: {recording.get_num_frames() / recording.get_sampling_frequency()} s\")\n",
    "        print(f\"Files Read: {file_counter}\")\n",
    "\n",
    "    #extractors.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a662de7",
   "metadata": {},
   "source": [
    "#                                       Ploting Signal Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b6d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_list_sorted = [\n",
    "    '14', '9', '11', '14', '15', '18', '20', '14',\n",
    "    '6',  '8', '10', '13', '16', '19', '21', '23',\n",
    "    '4',  '5',  '7', '12', '17', '22', '24', '25', \n",
    "    '1',  '0',  '2',  '3', '26', '27', '29', '28', \n",
    "    '58', '59', '57', '56', '33', '32', '30', '31', \n",
    "    '55', '54', '52', '47', '42', '37', '35', '34',\n",
    "    '53', '51', '49', '46', '43', '40', '38', '36', \n",
    "    '14', '50', '48', '45', '44', '41', '39', '14']\n",
    "\n",
    "# Calculate the size of the matrix\n",
    "n = int(np.sqrt(len(ch_list_sorted)))\n",
    "\n",
    "# Create a 2D matrix with the specified layout\n",
    "matrix = np.array(ch_list_sorted).reshape((n, n))\n",
    "\n",
    "# Rotate the matrix counter-clockwise by 90 degrees\n",
    "rotated_matrix = np.rot90(matrix, k=1)\n",
    "\n",
    "# Flatten the rotated matrix to get the desired list\n",
    "rotated_list = rotated_matrix.flatten()\n",
    "\n",
    "# Convert elements to strings\n",
    "rotated_list = [str(element) for element in rotated_list]\n",
    "\n",
    "print(rotated_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cbd549",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting selected channels. \n",
    "time_range = (50, 51)\n",
    "\n",
    "channel_list = ['53', '51', '49', '46', '43', '40', '38', '36']\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axs = plt.subplots(len(channel_list), 1, figsize=(40, 1 * len(channel_list)))\n",
    "\n",
    "# Iterate over channels and plot timeseries in each subplot\n",
    "for i, channel_id in enumerate(channel_list):\n",
    "    sw.plot_timeseries(recording,\n",
    "                      channel_ids=[channel_id],\n",
    "                      time_range=(50, 70),\n",
    "                      mode='auto',\n",
    "                      return_scaled=True,\n",
    "                      cmap='RdBu',\n",
    "                      show_channel_ids=True,\n",
    "                      color_groups=True,\n",
    "                      color=None,\n",
    "                      clim=None,\n",
    "                      tile_size=1500,\n",
    "                      seconds_per_row=0.2,\n",
    "                      with_colorbar=True,\n",
    "                      add_legend=None,\n",
    "                      backend=None,\n",
    "                      ax=axs[i])\n",
    "\n",
    "    # Set y-axis limits to fit the time series within the window\n",
    "    axs[i].set_ylim([-50, 50])  # Adjust this range as needed\n",
    "    \n",
    "    # Increase font size of channel_ids\n",
    "    axs[i].tick_params(axis='both', labelsize=40)  # Adjust the labelsize as needed\n",
    "\n",
    "    #axs[i].set_title(f'Ch {channel_id} Raw', fontsize=8)\n",
    "    # Remove x-axis label\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_xlabel('')\n",
    "    # Remove spines (borders)\n",
    "    axs[i].spines['top'].set_visible(False)\n",
    "    axs[i].spines['right'].set_visible(False)\n",
    "    axs[i].spines['bottom'].set_visible(False)\n",
    "    axs[i].spines['left'].set_visible(False)\n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0)  # Set spacing between subplots to 0\n",
    "plt.show()\n",
    "\n",
    "#################### Filterd traces ########################################################\n",
    "rec_f = spre.filter(recording, band=200, btype=\"highpass\", filter_mode=\"sos\", ftype='butter')\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axs = plt.subplots(len(channel_list), 1, figsize=(40, 1 * len(channel_list)))\n",
    "\n",
    "# Iterate over channels and plot timeseries in each subplot\n",
    "for i, channel_id in enumerate(channel_list):\n",
    "    sw.plot_timeseries(rec_f,\n",
    "                      channel_ids=[channel_id],\n",
    "                      time_range=(50, 70),\n",
    "                      mode='auto',\n",
    "                      return_scaled=True,\n",
    "                      cmap='RdBu',\n",
    "                      show_channel_ids=True,\n",
    "                      color_groups=True,\n",
    "                      color='blue',\n",
    "                      clim=None,\n",
    "                      tile_size=1500,\n",
    "                      seconds_per_row=0.2,\n",
    "                      with_colorbar=True,\n",
    "                      add_legend=None,\n",
    "                      backend=None,\n",
    "                      ax=axs[i])\n",
    "\n",
    "    # Set y-axis limits to fit the time series within the window\n",
    "    axs[i].set_ylim([-50, 50])  # Adjust this range as needed\n",
    "    \n",
    "    # Increase font size of channel_ids\n",
    "    axs[i].tick_params(axis='both', labelsize=40)  # Adjust the labelsize as needed\n",
    "\n",
    "    #axs[i].set_title(f'Ch {channel_id} Raw', fontsize=8)\n",
    "    # Remove x-axis label\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_xlabel('')\n",
    "    # Remove spines (borders)\n",
    "    axs[i].spines['top'].set_visible(False)\n",
    "    axs[i].spines['right'].set_visible(False)\n",
    "    axs[i].spines['bottom'].set_visible(False)\n",
    "    axs[i].spines['left'].set_visible(False)\n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0)  # Set spacing between subplots to 0\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f3a56",
   "metadata": {},
   "source": [
    "## Setting Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc9c07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = read_probeinterface('MCS_60channel_200_30_updated_contact_ids-11-88.json')\n",
    "plot_probe_group(probe,with_contact_id=True, with_device_index=True)\n",
    "\n",
    "# contact ids are the names of the electrodes as idicated on the MSC map and in MC_Rack. Device channel ids are the indices of the streams according to the wiring.\n",
    "\n",
    "probe_df=probe.to_dataframe(complete=True).loc[:, [\"contact_ids\", \"device_channel_indices\"]]\n",
    "display(probe_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49817635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contact ids are the names of the electrodes as indicated on the MSC map and in MC_Rack. \n",
    "# Device channel ids are the indices of the streams according to the wiring.\n",
    "probe_df=probe.to_dataframe(complete=True).loc[:, [\"contact_ids\", \"device_channel_indices\"]]\n",
    "probe_df[\"electrode_names\"]=electrode_names\n",
    "display(probe_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f8c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channels that will be excluded from sorting due to high noise levels\n",
    "\"\"\"\n",
    "exc_channels = [\n",
    "    [42,84],\n",
    "    [46]\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "exc_channels_ind = []\n",
    "for i in range(len(exc_channels)):\n",
    "    indices = []\n",
    "    for k in range(len(exc_channels[i])):\n",
    "        indices.append(probe_df[\"device_channel_indices\"][int(np.where(probe_df[\"contact_ids\"] == str(exc_channels[i][k]))[0])])\n",
    "    exc_channels_ind.append(indices)\n",
    "    \n",
    "print(exc_channels_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14442c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include channels \n",
    "\"\"\"\n",
    "inc_channels = [16, 17, 25, 26, 27, 28,\n",
    "                35, 36, 37, 38, 45, 46, 47, 48 \n",
    "                ]  \n",
    "inc_channels_ind = []\n",
    "\"\"\"\n",
    "## This part of code changed by Rola\n",
    "for i in inc_channels:\n",
    "    condition = probe_df[\"contact_ids\"] == str(i)\n",
    "    indices = np.where(condition)[0]  # This will still give us an array of indices\n",
    "    if len(indices) > 0:\n",
    "        # If there are multiple matches, this takes the first. Adjust accordingly.\n",
    "        index = indices[0]\n",
    "        inc_channels_ind.append(probe_df[\"device_channel_indices\"].iloc[index])\n",
    "    else:\n",
    "        # Handle the case where no match is found, if necessary\n",
    "        print(f\"No match found for {i}\")\n",
    "\n",
    "print(inc_channels_ind)\n",
    "inc_channels_ind.sort()\n",
    "print(inc_channels_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad456e1",
   "metadata": {},
   "source": [
    "# Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20535f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options for parallel processing:\n",
    "n_workers = 20\n",
    "chunk_memory = \"1000M\"\n",
    "\n",
    "# options for automatic curation/quality control\n",
    "snr_thresh = 5\n",
    "isi_viol_thresh = 0.2\n",
    "#query = f\"snr > {snr_thresh} & isi_violations_rate < {isi_viol_thresh}\"\n",
    "\n",
    "#digital data for stimulation timestamps\n",
    "n_digital_bits = 16\n",
    "input_stim_bit = 4 \n",
    "output_stim_bit = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80722880",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_job_kwargs = dict(n_jobs=20, chunk_duration=\"1s\")\n",
    "si.set_global_job_kwargs(**global_job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1593e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "se.recording_extractor_full_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eed9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### PROCESSING, SORTING and CURATION #################################################\n",
    "\n",
    "\n",
    "# load and preprocess data from folder\n",
    "\n",
    "for mea_file in mea_files:\n",
    "    file_path = os.path.join(directory_path, mea_file)\n",
    "    recording = se.MCSRawRecordingExtractor(mea_file)\n",
    "    print(recording)\n",
    "    recording.annotate(is_filtered=False)\n",
    "    channel_ids = recording.get_channel_ids()\n",
    "    fs = recording.get_sampling_frequency()\n",
    "    num_chan = recording.get_num_channels()\n",
    "    num_segments = recording.get_num_segments()\n",
    "    print(f'Channel ids: {channel_ids}')\n",
    "    print(f'Sampling frequency: {fs}')\n",
    "    print(f'Number of channels: {num_chan}')\n",
    "    print(f\"Number of segments: {num_segments}\")\n",
    "    recording_prb = recording.set_probes(probe)\n",
    "    recording_f = spre.filter(recording_prb, band=100, btype=\"highpass\", filter_mode=\"sos\", ftype='butter')\n",
    "    recording_cmr = spre.common_reference(recording_f, reference='global', operator='median')\n",
    "    rec_preprocessed = recording_cmr.save(folder=str(\"preprocessed_\" + str(mea_file)), progress_bar=True, n_jobs=n_workers, chunk_memory=chunk_memory)\n",
    "\n",
    "# sort preprocessed data using ironclust and save sorted data\n",
    "\n",
    "\n",
    "for mea_file in (mea_files):\n",
    "    print(mea_file)\n",
    "    recording_loaded = si.load_extractor(str(\"preprocessed_\" + str(mea_file) + \"/\"))\n",
    "    print (recording_loaded)\n",
    "\n",
    "    output_folder=str(\"results_IC_\" + str(mea_file))\n",
    "    print(output_folder)\n",
    "\n",
    "    recording_IC = ss.run_sorter('ironclust', \n",
    "                                 recording_loaded, \n",
    "                                 output_folder=str(\"results_IC_\" + str(mea_file)), \n",
    "                                 detect_threshold=5, \n",
    "                                 verbose=True)\n",
    "    \n",
    "    \n",
    "    recording_IC_saved = recording_IC.save(folder=str(\"sorting_IC_\" + str(mea_file)))\n",
    "\n",
    "\n",
    "#automatic curation according to SNR threshold and ISI violation (SNR set manually and ISI violation was not included)\n",
    "for mea_file in (mea_files):\n",
    "    recording_loaded = si.load_extractor(str(\"preprocessed_\" + str(mea_file) + \"/\"))\n",
    "    sorting_loaded = si.load_extractor('sorting_IC_' + str(mea_file) + \"/\")\n",
    "    recording_we = si.extract_waveforms(recording_loaded, sorting_loaded, folder= \"wf_\" + str(mea_file), progress_bar=True,n_jobs=n_workers, chunk_memory=chunk_memory, overwrite=True)\n",
    "    recording_qc = si.qualitymetrics.compute_quality_metrics(recording_we)\n",
    "    keep_units_recording = recording_qc.query(\"snr > 5 \")\n",
    "    keep_unit_ids_recording = keep_units_recording.index.values\n",
    "    recording_sorting_autocur = sorting_loaded.select_units(keep_unit_ids_recording)\n",
    "    recording_sorting_autocur.save(folder=str(\"autocurated_sorting_IC_\" + str(mea_file)))\n",
    "\n",
    "# expoprt timestamp information for each recording into a .csv file\n",
    "\n",
    "for mea_file in (mea_files):\n",
    "    sorting = si.load_extractor('autocurated_sorting_IC_' + str(mea_file) + \"/\")\n",
    "    n = len(sorting.get_unit_ids())\n",
    "    data = {}\n",
    "    for k in range(n):\n",
    "        data.update( {str(sorting.get_unit_ids()[k]) : sorting.get_unit_spike_train(sorting.get_unit_ids()[k])})\n",
    "    df = pd.DataFrame.from_dict(data,orient='index')\n",
    "    df = df.transpose()\n",
    "    df.to_csv('autocurated_timestamps_'+ str(mea_file) +'.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# extract unit ids and positions into csv file\n",
    "for mea_file in mea_files:\n",
    "    # Load preprocessed recording and autocurated sorting\n",
    "    sorting = si.load_extractor('autocurated_sorting_IC_' + str(mea_file) + \"/\")\n",
    "    si.load_extractor(str(\"preprocessed_\" + str(mea_file) + \"/\"))\n",
    "    recording_we = si.extract_waveforms(recording_loaded, sorting_loaded, folder= \"wf_\" + str(mea_file), progress_bar=True,n_jobs=n_workers, chunk_memory=chunk_memory, overwrite=True)\n",
    "    #recording_loaded = si.load_extractor(os.path.join(directory_path, f\"preprocessed_{mea_file}/\"))\n",
    "    \n",
    "    \n",
    "    # Extract unit IDs and positions\n",
    "    unit_ids = sorting.get_unit_ids()\n",
    "    unit_loc=spost.compute_unit_locations(recording_we)\n",
    "    \n",
    "    # Prepare data for DataFrame\n",
    "    data = {'unit_id': [], 'x': [], 'y': [], 'z': []}\n",
    "    for unit_id, position in zip(unit_ids, unit_loc):\n",
    "        data['unit_id'].append(unit_id)\n",
    "        data['x'].append(position[0])  # Assuming x is the first element in the position tuple\n",
    "        data['y'].append(position[1])  # Assuming y is the second element in the position tuple\n",
    "        data['z'].append(position[2])  # Assuming z is the third element in the position tuple\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Export to CSV\n",
    "    df.to_csv(os.path.join(directory_path, f\"unit_positions_{mea_file}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e2b3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29d20c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract unit ids and positions into csv file\n",
    "for mea_file in mea_files:\n",
    "    # Load preprocessed recording and autocurated sorting\n",
    "\n",
    "    recording_loaded = si.load_extractor(str(\"preprocessed_\" + str(mea_file) + \"/\"))\n",
    "    sorting_autocur_loaded = si.load_extractor('autocurated_sorting_IC_' + str(mea_file) + \"/\")\n",
    "    recording_we = si.extract_waveforms(recording_loaded, sorting_autocur_loaded, folder= \"wf_\" + str(mea_file), progress_bar=True,n_jobs=n_workers, chunk_memory=chunk_memory, overwrite=True)\n",
    "    \n",
    "    # Extract unit IDs and positions\n",
    "    unit_ids = sorting_autocur_loaded.get_unit_ids()\n",
    "    print(str(mea_file), unit_ids, 'number of units:', len(unit_ids))\n",
    "    unit_loc=spost.compute_unit_locations(recording_we)\n",
    "    print(str(mea_file), unit_loc, 'loc:', len(unit_loc))\n",
    "    \n",
    "    \n",
    "    # Prepare data for DataFrame\n",
    "    data = {'unit_id': [], 'x': [], 'y': [], 'z': []}\n",
    "    print(r'data', data)\n",
    "    for unit_id, position in zip(unit_ids, unit_loc):\n",
    "        data['unit_id'].append(unit_id)\n",
    "        data['x'].append(position[0])  # Assuming x is the first element in the position tuple\n",
    "        data['y'].append(position[1])  # Assuming y is the second element in the position tuple\n",
    "        data['z'].append(position[2])  # Assuming z is the third element in the position tuple\n",
    "    \n",
    "    print(r'unit_id', unit_id)\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    print(r'df', df, 'number of units:', len(df))\n",
    "    \n",
    "    # Export to CSV\n",
    "    df.to_csv(os.path.join(directory_path, f\"unit_positions_{mea_file}.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
